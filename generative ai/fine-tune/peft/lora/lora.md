- Low-rank Adaptation (LoRA) là một kỹ thuật tinh chỉnh hiệu quả tham số thuộc loại tái tham số hóa.
![[Pasted image 20240701133746.png]]
sơ đồ của kiến trúc [[transformers model]] mà bạn đã thấy trước đó trong khóa học. [[prompt]] đầu vào được chuyển thành các [[tokens]], sau đó được chuyển đổi thành các [[vector embedding]] và đưa vào phần [[encoder]] và/hoặc [[decoder]] của transformer.

Trong cả hai thành phần này, có hai loại mạng neural: [[self-attention]] và [[feedforward network]]. Trọng số của các mạng này được học trong quá trình tiền huấn luyện. Sau khi các [[vector embedding]] được tạo ra, chúng được đưa vào các [[self-attention class]], nơi một loạt các trọng số được áp dụng để tính toán các điểm attention. Trong quá trình tinh chỉnh hoàn chỉnh, mọi tham số trong các lớp này đều được cập nhật.

![[Pasted image 20240701134512.png]]
LoRA là một chiến lược giảm số lượng tham số cần được huấn luyện trong quá trình tinh chỉnh bằng cách đóng băng tất cả các tham số mô hình gốc và sau đó thêm một cặp ma trận phân rã hạng vào cùng với trọng số gốc.
- Kích thước của các ma trận nhỏ hơn này được đặt sao cho sản phẩm của chúng là một ma trận có cùng kích thước với các trọng số mà chúng đang thay đổi. Bạn sau đó giữ nguyên trọng số gốc của LLM và huấn luyện các ma trận nhỏ hơn bằng cách sử dụng cùng quá trình học có giám sát
![[Pasted image 20240701134644.png]]
Đối với suy luận, hai ma trận hạng thấp được nhân với nhau để tạo ra một ma trận có cùng kích thước với các trọng số bị đóng băng. Sau đó, bạn cộng ma trận này vào các trọng số gốc và thay thế chúng trong mô hình với các giá trị cập nhật này.
- Bạn bây giờ có một mô hình LoRA đã được tinh chỉnh có thể thực hiện nhiệm vụ cụ thể của bạn. Vì mô hình này có cùng số lượng tham số như mô hình gốc, nên có ít hoặc không có tác động đến độ trễ suy luận.
Các nhà nghiên cứu đã phát hiện ra rằng áp dụng LoRA chỉ vào các lớp self-attention của mô hình thường đủ để tinh chỉnh cho một nhiệm vụ và đạt được các cải tiến về hiệu suất. Tuy nhiên, về nguyên tắc, bạn cũng có thể sử dụng LoRA trên các thành phần khác như các lớp feed-forward. Nhưng vì hầu hết các tham số của LLM nằm trong các lớp attention, bạn sẽ tiết kiệm được nhiều nhất trong các tham số có thể huấn luyện bằng cách áp dụng LoRA vào các ma trận trọng số này.

[1] A. Vaswani _et al._, “Attention Is All You Need.” arXiv, Aug. 01, 2023. 
- Bài báo chỉ định rằng các trọng số của transformer có kích thước 512 x 64. Điều này có nghĩa là mỗi ma trận trọng số có 32,768 tham số có thể huấn luyện. Nếu bạn sử dụng LoRA như một phương pháp tinh chỉnh với hạng bằng tám, bạn sẽ huấn luyện hai ma trận phân rã hạng nhỏ có kích thước nhỏ là tám. 
- Điều này có nghĩa là Ma trận A sẽ có kích thước 8 x 64, dẫn đến tổng cộng 512 tham số. Ma trận B sẽ có kích thước 512 x 8, hoặc 4,096 tham số có thể huấn luyện. Bằng cách cập nhật các trọng số của các ma trận hạng thấp mới này thay vì trọng số gốc, bạn sẽ huấn luyện 4,608 tham số thay vì 32,768, giảm 86%. 
- Vì LoRA cho phép bạn giảm đáng kể số lượng tham số có thể huấn luyện, bạn thường có thể thực hiện phương pháp tinh chỉnh hiệu quả tham số này chỉ với một GPU và tránh cần một cụm GPU phân tán.

Vì các ma trận phân rã hạng nhỏ, bạn có thể tinh chỉnh một tập hợp khác nhau cho mỗi nhiệm vụ và sau đó hoán đổi chúng trong quá trình suy luận bằng cách cập nhật trọng số. 
- Giả sử bạn huấn luyện một cặp ma trận LoRA cho một nhiệm vụ cụ thể; gọi nó là Nhiệm vụ A. Để thực hiện suy luận trên nhiệm vụ này, bạn sẽ nhân các ma trận này với nhau và sau đó cộng ma trận kết quả vào các trọng số gốc. Sau đó, bạn lấy ma trận trọng số mới này và thay thế trọng số gốc nơi chúng xuất hiện trong mô hình của bạn. Bạn sau đó có thể sử dụng mô hình này để thực hiện suy luận trên Nhiệm vụ A.
- Nếu thay vào đó, bạn muốn thực hiện một nhiệm vụ khác, chẳng hạn như Nhiệm vụ B, bạn chỉ cần lấy các ma trận LoRA mà bạn đã huấn luyện cho nhiệm vụ này, tính toán sản phẩm của chúng và sau đó cộng ma trận này vào các trọng số gốc và cập nhật mô hình một lần nữa. Bộ nhớ cần thiết để lưu trữ các ma trận LoRA này rất nhỏ. Vì vậy, về nguyên tắc, bạn có thể sử dụng LoRA để huấn luyện cho nhiều nhiệm vụ. Thay đổi các trọng số khi bạn cần sử dụng chúng và tránh phải lưu trữ nhiều phiên bản đầy đủ kích thước của LLM.

Các mô hình này tốt đến mức nào? 
Hãy sử dụng chỉ số ROUGE để so sánh hiệu suất của một mô hình tinh chỉnh LoRA với cả mô hình gốc và phiên bản tinh chỉnh hoàn chỉnh. Hãy tập trung vào việc tinh chỉnh FLAN-T5 cho tóm tắt hội thoại, mà bạn đã khám phá trước đó trong tuần. Như một lời nhắc nhở, mô hình FLAN-T5-base đã có một bộ tinh chỉnh hoàn chỉnh ban đầu được thực hiện bằng cách sử dụng một bộ dữ liệu hướng dẫn lớn. Trước tiên, hãy thiết lập một điểm số cơ sở cho mô hình FLAN-T5 gốc và bộ dữ liệu tóm tắt mà chúng ta đã thảo luận trước đó. Dưới đây là các điểm số ROUGE cho mô hình gốc, trong đó số cao hơn cho thấy hiệu suất tốt hơn. Bạn nên tập trung vào chỉ số ROUGE 1 cho cuộc thảo luận này, nhưng bạn có thể sử dụng bất kỳ điểm số nào trong số này để so sánh. Như bạn có thể thấy, các điểm số khá thấp.
![[Pasted image 20240701141411.png]]
Tiếp theo, hãy xem các điểm số cho một mô hình đã có thêm tinh chỉnh hoàn chỉnh trên tóm tắt hội thoại. Hãy nhớ rằng mặc dù FLAN-T5 là một mô hình có khả năng, nó vẫn có thể được hưởng lợi từ việc tinh chỉnh thêm trên các nhiệm vụ cụ thể. Với tinh chỉnh hoàn chỉnh, bạn cập nhật mọi trọng số trong mô hình trong quá trình học có giám sát. Bạn có thể thấy rằng điều này dẫn đến một điểm số ROUGE 1 cao hơn nhiều, tăng so với mô hình FLAN-T5 gốc là 0.19. Vòng tinh chỉnh bổ sung đã cải thiện đáng kể hiệu suất của mô hình trên nhiệm vụ tóm tắt.

Bây giờ hãy xem các điểm số cho mô hình tinh chỉnh LoRA. Bạn có thể thấy rằng quá trình này cũng dẫn đến một sự gia tăng lớn về hiệu suất. Điểm số ROUGE 1 đã tăng từ cơ sở là 0.17. Điều này thấp hơn một chút so với tinh chỉnh hoàn chỉnh, nhưng không nhiều. Tuy nhiên, việc sử dụng LoRA để tinh chỉnh đã huấn luyện một số lượng tham số nhỏ hơn nhiều so với tinh chỉnh hoàn chỉnh, sử dụng ít tính toán hơn đáng kể, vì vậy sự đánh đổi nhỏ này về hiệu suất có thể rất đáng giá.

Về nguyên tắc, hạng càng nhỏ, số lượng tham số có thể huấn luyện càng nhỏ, và tiết kiệm tính toán càng lớn. Tuy nhiên, có một số vấn đề liên quan đến hiệu suất mô hình cần xem xét. Trong bài báo đầu tiên đề xuất LoRA, các nhà nghiên cứu tại Microsoft đã khám phá cách các lựa chọn hạng khác nhau ảnh hưởng đến hiệu suất mô hình trên các nhiệm vụ tạo ngôn ngữ. Bạn có thể xem tóm tắt kết quả trong bảng ở đây.
![[Pasted image 20240701141522.png]]
Các giá trị in đậm cho thấy các điểm số tốt nhất đã đạt được cho mỗi chỉ số. Các tác giả đã tìm thấy một điểm bão hòa trong giá trị mất mát cho các hạng lớn hơn 16. Nói cách khác, việc sử dụng các ma trận LoRA lớn hơn không cải thiện hiệu suất. Kết luận ở đây là các hạng trong khoảng từ 4-32 có thể cung cấp cho bạn một sự đánh đổi tốt giữa việc giảm các tham số có thể huấn luyện và duy trì hiệu suất.